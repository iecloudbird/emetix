"""
Comprehensive Evaluation of Traditional ML Models
Tests: Linear Valuation, RF Ensemble, Risk Classifier

Usage:
    python scripts/evaluate_traditional_models.py
    python scripts/evaluate_traditional_models.py --output-report
"""
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

import argparse
import time
import pandas as pd
import numpy as np
from typing import Dict, List
import json

from src.models.valuation.linear_valuation import LinearValuationModel
from src.models.risk.risk_classifier import RiskClassifier
from src.data.fetchers.yfinance_fetcher import YFinanceFetcher
from config.settings import MODELS_DIR
from config.logging_config import get_logger

logger = get_logger(__name__)

# Test stocks representing different sectors
TEST_STOCKS = [
    'AAPL',  # Technology (Large Cap)
    'MSFT',  # Technology (Large Cap)
    'GOOGL', # Technology (Large Cap)
    'JPM',   # Finance
    'JNJ',   # Healthcare
    'XOM',   # Energy
    'WMT',   # Consumer
    'BA',    # Industrial
]


class TraditionalModelEvaluator:
    """Evaluates traditional ML models (Linear, RF, Risk)"""
    
    def __init__(self):
        self.fetcher = YFinanceFetcher()
        self.results = {
            'linear_valuation': {},
            'risk_classifier': {},
            'performance': {},
            'srs_compliance': {}
        }
    
    def load_models(self):
        """Load all traditional models"""
        logger.info("Loading traditional models...")
        
        try:
            # Linear Valuation Model
            self.linear_model = LinearValuationModel()
            linear_path = MODELS_DIR / "linear_valuation.pkl"
            if linear_path.exists():
                self.linear_model.load(str(linear_path))
                logger.info("âœ… Linear Valuation Model loaded")
            else:
                logger.warning("âš ï¸ Linear Valuation Model not found")
                self.linear_model = None
            
            # Risk Classifier
            self.risk_model = RiskClassifier()
            risk_path = MODELS_DIR / "risk_classifier.pkl"
            if risk_path.exists():
                self.risk_model.load(str(risk_path))
                logger.info("âœ… Risk Classifier loaded")
            else:
                logger.warning("âš ï¸ Risk Classifier not found")
                self.risk_model = None
            
            # Note: RF Ensemble requires separate loading (not implemented here)
            # TODO: Add RF Ensemble evaluation when model structure is confirmed
            
        except Exception as e:
            logger.error(f"Error loading models: {e}")
            raise
    
    def evaluate_linear_valuation(self, ticker: str) -> Dict:
        """Evaluate Linear Valuation Model on a stock"""
        if not self.linear_model:
            return {'error': 'Model not loaded'}
        
        try:
            # Fetch stock data
            stock_data = self.fetcher.fetch_stock_data(ticker)
            if stock_data is None or (isinstance(stock_data, pd.DataFrame) and stock_data.empty):
                return {'error': 'No data available'}
            
            # Prepare features
            features = {
                'pe_ratio': stock_data.get('pe_ratio', 0),
                'debt_to_equity': stock_data.get('debt_to_equity', 0),
                'revenue_growth': stock_data.get('revenue_growth', 0),
                'beta': stock_data.get('beta', 1.0)
            }
            
            # Time the inference (SRS NFR-ML-1: < 300ms)
            start_time = time.time()
            fair_value = self.linear_model.predict(features)
            inference_time = (time.time() - start_time) * 1000  # ms
            
            # Get current price for comparison
            current_price = stock_data.get('current_price', 0)
            
            result = {
                'ticker': ticker,
                'fair_value': fair_value,
                'current_price': current_price,
                'upside_pct': ((fair_value - current_price) / current_price * 100) if current_price > 0 else 0,
                'features': features,
                'inference_time_ms': inference_time,
                'meets_srs_nfr_ml1': inference_time < 300
            }
            
            logger.info(f"{ticker}: Fair Value=${fair_value:.2f}, Current=${current_price:.2f}, "
                       f"Upside={result['upside_pct']:.1f}%, Time={inference_time:.1f}ms")
            
            return result
            
        except Exception as e:
            logger.error(f"Error evaluating {ticker}: {e}")
            return {'error': str(e)}
    
    def evaluate_risk_classifier(self, ticker: str) -> Dict:
        """Evaluate Risk Classifier on a stock"""
        if not self.risk_model:
            return {'error': 'Model not loaded'}
        
        try:
            # Fetch stock data
            stock_data = self.fetcher.fetch_stock_data(ticker)
            if stock_data is None or (isinstance(stock_data, pd.DataFrame) and stock_data.empty):
                return {'error': 'No data available'}
            
            # Prepare features
            features = {
                'beta': stock_data.get('beta', 1.0),
                'debt_to_equity': stock_data.get('debt_to_equity', 0),
                'current_ratio': stock_data.get('current_ratio', 1.0)
            }
            
            # Time the inference
            start_time = time.time()
            risk_level = self.risk_model.predict(features)
            inference_time = (time.time() - start_time) * 1000  # ms
            
            result = {
                'ticker': ticker,
                'risk_level': risk_level,
                'beta': features['beta'],
                'debt_to_equity': features['debt_to_equity'],
                'inference_time_ms': inference_time,
                'meets_srs_nfr_ml1': inference_time < 300
            }
            
            logger.info(f"{ticker}: Risk={risk_level}, Beta={features['beta']:.2f}, "
                       f"D/E={features['debt_to_equity']:.2f}, Time={inference_time:.1f}ms")
            
            return result
            
        except Exception as e:
            logger.error(f"Error evaluating {ticker}: {e}")
            return {'error': str(e)}
    
    def run_comprehensive_evaluation(self, test_stocks: List[str] = None):
        """Run comprehensive evaluation on test stocks"""
        if test_stocks is None:
            test_stocks = TEST_STOCKS
        
        logger.info("\n" + "="*80)
        logger.info("TRADITIONAL MODELS EVALUATION")
        logger.info("="*80)
        
        # Evaluate Linear Valuation
        logger.info("\nðŸ“Š Evaluating Linear Valuation Model...")
        linear_results = []
        for ticker in test_stocks:
            result = self.evaluate_linear_valuation(ticker)
            if 'error' not in result:
                linear_results.append(result)
        
        self.results['linear_valuation'] = linear_results
        
        # Evaluate Risk Classifier
        logger.info("\nðŸŽ¯ Evaluating Risk Classifier...")
        risk_results = []
        for ticker in test_stocks:
            result = self.evaluate_risk_classifier(ticker)
            if 'error' not in result:
                risk_results.append(result)
        
        self.results['risk_classifier'] = risk_results
        
        # Calculate aggregate metrics
        self._calculate_aggregate_metrics()
        
        # Check SRS compliance
        self._check_srs_compliance()
        
        # Print summary
        self._print_summary()
    
    def _calculate_aggregate_metrics(self):
        """Calculate aggregate performance metrics"""
        metrics = {}
        
        # Linear Valuation metrics
        if self.results['linear_valuation']:
            linear_times = [r['inference_time_ms'] for r in self.results['linear_valuation']]
            metrics['linear_avg_inference_time'] = np.mean(linear_times)
            metrics['linear_max_inference_time'] = np.max(linear_times)
            metrics['linear_passes_nfr_ml1'] = all(
                r['meets_srs_nfr_ml1'] for r in self.results['linear_valuation']
            )
        
        # Risk Classifier metrics
        if self.results['risk_classifier']:
            risk_times = [r['inference_time_ms'] for r in self.results['risk_classifier']]
            metrics['risk_avg_inference_time'] = np.mean(risk_times)
            metrics['risk_max_inference_time'] = np.max(risk_times)
            metrics['risk_passes_nfr_ml1'] = all(
                r['meets_srs_nfr_ml1'] for r in self.results['risk_classifier']
            )
        
        self.results['performance'] = metrics
    
    def _check_srs_compliance(self):
        """Check compliance with SRS requirements"""
        compliance = {
            'NFR-ML-1': {  # ML inference < 300ms
                'requirement': 'ML inference shall complete in < 300ms per stock',
                'linear_valuation': self.results['performance'].get('linear_passes_nfr_ml1', False),
                'risk_classifier': self.results['performance'].get('risk_passes_nfr_ml1', False),
                'status': 'PASS' if (
                    self.results['performance'].get('linear_passes_nfr_ml1', False) and
                    self.results['performance'].get('risk_passes_nfr_ml1', False)
                ) else 'FAIL'
            },
            'NFR-ML-4': {  # Graceful fallback
                'requirement': 'Models shall gracefully fallback if unavailable',
                'status': 'PASS',  # Handled by error checking
                'notes': 'Error handling implemented, returns error dict instead of crashing'
            }
        }
        
        self.results['srs_compliance'] = compliance
    
    def _print_summary(self):
        """Print evaluation summary"""
        logger.info("\n" + "="*80)
        logger.info("ðŸ“Š EVALUATION SUMMARY")
        logger.info("="*80)
        
        # Linear Valuation Summary
        if self.results['linear_valuation']:
            logger.info("\nâœ… Linear Valuation Model:")
            logger.info(f"  Stocks evaluated: {len(self.results['linear_valuation'])}")
            logger.info(f"  Avg inference time: {self.results['performance']['linear_avg_inference_time']:.2f}ms")
            logger.info(f"  Max inference time: {self.results['performance']['linear_max_inference_time']:.2f}ms")
            logger.info(f"  NFR-ML-1 (< 300ms): {'âœ… PASS' if self.results['performance']['linear_passes_nfr_ml1'] else 'âŒ FAIL'}")
            
            # Show sample predictions
            logger.info("\n  Sample Predictions:")
            for result in self.results['linear_valuation'][:3]:
                logger.info(f"    {result['ticker']}: ${result['fair_value']:.2f} "
                           f"(Current: ${result['current_price']:.2f}, "
                           f"Upside: {result['upside_pct']:+.1f}%)")
        
        # Risk Classifier Summary
        if self.results['risk_classifier']:
            logger.info("\nâœ… Risk Classifier:")
            logger.info(f"  Stocks evaluated: {len(self.results['risk_classifier'])}")
            logger.info(f"  Avg inference time: {self.results['performance']['risk_avg_inference_time']:.2f}ms")
            logger.info(f"  Max inference time: {self.results['performance']['risk_max_inference_time']:.2f}ms")
            logger.info(f"  NFR-ML-1 (< 300ms): {'âœ… PASS' if self.results['performance']['risk_passes_nfr_ml1'] else 'âŒ FAIL'}")
            
            # Show sample classifications
            logger.info("\n  Sample Classifications:")
            for result in self.results['risk_classifier'][:3]:
                logger.info(f"    {result['ticker']}: {result['risk_level']} "
                           f"(Beta: {result['beta']:.2f})")
        
        # SRS Compliance
        logger.info("\nðŸ“‹ SRS Compliance:")
        for req_id, req_data in self.results['srs_compliance'].items():
            status_icon = "âœ…" if req_data['status'] == 'PASS' else "âŒ"
            logger.info(f"  {req_id}: {status_icon} {req_data['status']}")
        
        logger.info("="*80)
    
    def save_results(self, output_path: str = None):
        """Save evaluation results to JSON"""
        if output_path is None:
            output_path = Path(__file__).parent.parent / "data" / "evaluation" / "traditional_models_eval.json"
        
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, 'w') as f:
            json.dump(self.results, f, indent=2, default=str)
        
        logger.info(f"\nðŸ’¾ Results saved to: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='Evaluate traditional ML models')
    parser.add_argument('--stocks', nargs='+', help='Stocks to evaluate (default: predefined test set)')
    parser.add_argument('--output-report', action='store_true', help='Save evaluation report')
    args = parser.parse_args()
    
    # Create evaluator
    evaluator = TraditionalModelEvaluator()
    
    # Load models
    evaluator.load_models()
    
    # Run evaluation
    test_stocks = args.stocks if args.stocks else TEST_STOCKS
    evaluator.run_comprehensive_evaluation(test_stocks)
    
    # Save results
    if args.output_report:
        evaluator.save_results()


if __name__ == "__main__":
    main()
